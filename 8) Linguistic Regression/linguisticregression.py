# -*- coding: utf-8 -*-
"""LinguisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fuHuUzmAZ9hXYPsW0jbUsxZ71KLYuJ1P

Linguistic Research for "datast_tr.csv"
"""

import pandas as pd


# Loading dataset
data_path = 'news.csv'
df = pd.read_csv(data_path, error_bad_lines=False)

# Calculating word numbers for each body
df["character_count"] = df["Body"].apply(lambda x: len(str(x)))
df["word_count"] = df["Body"].apply(lambda x: len(str(x).split()))
linguistic_data = df[["Body", "Label", "character_count", "word_count"]]

df["letter_count"] = df["Body"].apply(lambda x: len([c for c in x if c.isalpha()]))

df["total_number_of_upper_characters"] = df["Body"].apply(lambda x: sum(1 for c in x if c.isupper()))

df["total_number_of_lower_characters"] = df["Body"].apply(lambda x: sum(1 for c in x if c.islower()))

df.head()

import string

df["number_of_special_character"] = df["Body"].apply(lambda x: sum(1 for c in x if c in string.punctuation))

df["short_words"] = df["Body"].apply(lambda x: len([w for w in x.split() if len(w)<=4]))

df["long_words"] = df["Body"].apply(lambda x: len([w for w in x.split() if len(w)>4]))

df["number_of_different_words"] = df["Body"].apply(lambda x: len(set(x.split())))

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
#nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import locale
locale.getdefaultlocale()

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install stanfordnlp

import stanfordnlp
stanfordnlp.download('tr')

nlp = stanfordnlp.Pipeline(processors='tokenize,pos', lang='tr')

unique_word_types_count = []
for article in df['Body']:
    word_types = set()
    doc = nlp(article)
    for sentence in doc.sentences:
        for word in sentence.words:
            if word.upos not in ['PUNCT', 'SYM', 'X']:
                word_types.add(word.upos)
    unique_word_types_count.append(len(word_types))

# Store the results in a new column
df['unique_word_types_count'] = unique_word_types_count

def get_verbs(text):
    doc = nlp(text)
    verbs = [word.text for sent in doc.sentences for word in sent.words if word.upos == 'VERB']
    return verbs

import stanfordnlp
def count_verbs(text):
    verbs = get_verbs(text)
    return len(verbs)

df['verb_count'] = df['Body'].apply(count_verbs)

def count_adjectives(text):
    doc = nlp(text)
    adjectives = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'ADJ']
    return len(adjectives)

df['adjective_count'] = df['Body'].apply(count_adjectives)

def preprocess_text(text):
    # Tokenize 
    doc = nlp(text)
    tokens = [word.text for sent in doc.sentences for word in sent.words]
    
    # Remove stop words
    tokens = [token for token in tokens if token.lower() not in stop_words]
    
    # Split words into roots
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    return lemmatized_tokens

df['determiners_count'] = df['Body'].apply(lambda x: len([word.upos for sent in nlp(x).sentences for word in sent.words if word.upos == 'DET']))

# Calculate the number of conjunctions
def count_conjunctions(text):
    doc = nlp(text)
    conjunctions = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'CCONJ']
    return len(conjunctions)

df['conjunction_count'] = df['Body'].apply(count_conjunctions)

def count_nouns(text):
    doc = nlp(text)
    nouns = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'NOUN']
    return len(nouns)

df['noun_count'] = df['Body'].apply(count_nouns)

df.head()

def count_adverbs(text):
    doc = nlp(text)
    adverbs = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'ADV']
    return len(adverbs)

df['adverb_count'] = df['Body'].apply(count_adverbs)

def count_prepositions(text):
    doc = nlp(text)
    prepositions = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'ADP']
    return len(prepositions)

df['preposition_count'] = df['Body'].apply(count_prepositions)

def count_pronouns(text):
    doc = nlp(text)
    pronouns = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == 'PRON']
    return len(pronouns)
df['pronoun_count'] = df['Body'].apply(count_pronouns)

df.head()#Check

import re
def count_numbers(text):
    doc = nlp(text)
    numbers = re.findall(r'\d+', text)
    return len(numbers)
df['number_count'] = df['Body'].apply(count_numbers)

"""Sentiment analysis of each news"""

!pip install transformers

import transformers

from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
model = AutoModelForSequenceClassification.from_pretrained("savasy/bert-base-turkish-sentiment-cased")
tokenizer = AutoTokenizer.from_pretrained("savasy/bert-base-turkish-sentiment-cased", max_length=1024)

sa= pipeline("sentiment-analysis", tokenizer=tokenizer, model=model)

df['sentiment'] = df['Body'].apply(lambda x: sa(x[:512])[0]['label'])

linguistic_data = df[["Body","Label", "character_count", "word_count", "letter_count",
                      "total_number_of_upper_characters", "total_number_of_lower_characters",
                      "number_of_special_character", "short_words", "long_words", "number_of_different_words","unique_word_types_count"
                      ,"verb_count","adjective_count","determiners_count","conjunction_count","noun_count",
                      "adverb_count","preposition_count","pronoun_count",
                      "number_count","sentiment"]]

df.head(30)

print(linguistic_data.head())

"""Adding "NER" Models...

"""

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy
import pandas as pd

# "en_core_web_sm" 
nlp = spacy.load("en_core_web_sm")

# List for entities.
entities = []

# Apply NER for "Body" column.
for text in df["Body"]:
    doc = nlp(text)
    for ent in doc.ents:
        entities.append((ent.text, ent.label_))
        
# Dataframe.
entities_df = pd.DataFrame(entities, columns=["entity", "label"])

print(entities_df.groupby("label").count())

"""This output contains numbers according to the entity types determined by the NER model.


*   CARDINAL: numeric values

*   DATE: dates

*   EVENT: events

*   FAC: structures and facilities

*   GPE: countries, cities and regions

*   LANGUAGE: languages

*   LAW: legal terms

*   LOC: geolocations

*   MONEY: currencies

*   NORP: nationality and ethnic groups

*   ORDINAL: rank numbers

*   ORG: companies, organizations and institutions

*   PERCENT: percentage values

*   PERSON: persons

*   PRODUCT: products and services

*   QUANTITY: units of measure

*   TIME: hours, days, etc. time expressions

*   WORK_OF_ART: books, movies, etc. artworks



For example, the most detected entity type in this dataset was "PERSON" and there are 152532 "PERSON" entities in total.
"""

import spacy
import pandas as pd

# Load English NLP model
nlp = spacy.load("en_core_web_sm")

# Define function to get entity counts from a text string
def get_entity_counts(row):
    doc = nlp(row["Body"])
    entity_counts = {label:0 for label in nlp.pipe_labels["ner"]}
    for ent in doc.ents:
        entity_counts[ent.label_] += 1
    return pd.Series(entity_counts)

# Read in the data
df = linguistic_data[[ "Body","Label", "character_count", "word_count", "letter_count",
                      "total_number_of_upper_characters", "total_number_of_lower_characters",
                      "number_of_special_character", "short_words", "long_words", "number_of_different_words","unique_word_types_count"
                      ,"verb_count","adjective_count","determiners_count","conjunction_count","noun_count",
                      "adverb_count","preposition_count","pronoun_count",
                      "number_count","sentiment"]]

# Apply the function to each row of the dataframe to get entity counts
entity_counts = df.apply(get_entity_counts, axis=1)

# Combine the entity counts with the original dataframe
new_data = pd.concat([df, entity_counts], axis=1)

print(new_data.head())

"""----------------

"""

new_data.to_csv('new_linguistic_data.csv')