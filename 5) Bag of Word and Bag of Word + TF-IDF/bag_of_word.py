# -*- coding: utf-8 -*-
"""Bag_of_Word.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-PrBgaGj5AMclQ2N_iC9-aJ-nVC3hpx

## BAG OF WORD

## bag of word
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
import nltk
nltk.download('punkt')

data_path = 'clean_news.csv'
df = pd.read_csv(data_path, error_bad_lines=False)

df.head()

# Creating bag of words using CountVectorizer 
count_vectorizer = CountVectorizer(analyzer="word", tokenizer=nltk.word_tokenize, preprocessor=None)
bag_of_words = count_vectorizer.fit_transform(df['Body'])

# Setting class labels as targets
label = df['Label'].values

# Train-test split 
from sklearn.model_selection import train_test_split

# Generate training and testing data
X_train, X_test, y_train, y_test = train_test_split(bag_of_words, label, test_size=0.20, random_state=42)

print(count_vectorizer.vocabulary_)
len(count_vectorizer.vocabulary_)

print(bag_of_words.toarray())

"""### LOGISTIC REGRESSION"""

# Creating and training a Logistic Regression model
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(max_iter=10000)
classifier.fit(X_train, y_train)

# Make predictions using test data
y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate performance metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

"""### DECISION TREE"""

# Creating the Decision Tree model
dt = DecisionTreeClassifier()

# Train the model
dt.fit(X_train, y_train)

# Make prediction on test data
y_pred_dt = dt.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_dt, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_dt, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))

"""### RANDOM FOREST"""

# Creating the Random Forest model

rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make prediction on test data
rf_y_pred = rf.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, rf_y_pred))
print("Precision:", precision_score(y_test, rf_y_pred, average='macro'))
print("Recall:", recall_score(y_test, rf_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, rf_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, rf_y_pred))
print("Classification Report:")
print(classification_report(y_test, rf_y_pred))

"""### GRADIENT BOOSTING"""

# Creating the Gradient Boosting model
gb = GradientBoostingClassifier()

# Train the model
gb.fit(X_train, y_train)

# Make prediction on test data
y_pred_gb = gb.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print("Precision:", precision_score(y_test, y_pred_gb, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_gb, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_gb, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_gb))
print("Classification Report:")
print(classification_report(y_test, y_pred_gb))

"""### SUPPORT VECTOR MACHINE"""

# Creating the Support Vector Machine model
svm = SVC()

# Train the model
svm.fit(X_train, y_train)

# Make prediction on test data
y_pred_svm = svm.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Precision:", precision_score(y_test, y_pred_svm, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_svm, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_svm, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))

"""### KNN"""

# Creating the KNN model
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(X_train, y_train)

# Make prediction on test data
knn_y_pred = knn.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, knn_y_pred))
print("Precision:", precision_score(y_test, knn_y_pred, average='macro'))
print("Recall:", recall_score(y_test, knn_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, knn_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, knn_y_pred))
print("Classification Report:")
print(classification_report(y_test, knn_y_pred))

"""### NAIVE BAYES"""

# Creating the Naive Bayes model
nb = MultinomialNB()

# Train the model
nb.fit(X_train, y_train)

#Make prediction on test data
nb_y_pred = nb.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, nb_y_pred))
print("Precision:", precision_score(y_test, nb_y_pred, average='macro'))
print("Recall:", recall_score(y_test, nb_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, nb_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, nb_y_pred))
print("Classification Report:")
print(classification_report(y_test, nb_y_pred))

"""# BAG OF WORD + TFIDF

## bag of word + tfidf
"""

data_path = 'clean_news.csv'
df1 = pd.read_csv(data_path, error_bad_lines=False)

df1.head()

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

# Generate BoW vectors using CountVectorizer
count_vectorizer2 = CountVectorizer()
bow_vectors = count_vectorizer2.fit_transform(df1['Body'])

# Converting BoW vectors to TF-IDF vectors using TfidfTransformer
tfidf_transformer = TfidfTransformer()
tfidf_vectors = tfidf_transformer.fit_transform(bow_vectors)

len(count_vectorizer2.vocabulary_)

# Setting class labels as targets
labels = df1['Label'].values

# Train-test split 
from sklearn.model_selection import train_test_split

# Generate training and testing data
X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.20, random_state=42)

print(count_vectorizer2.vocabulary_)

"""## Logistic Regression"""

# Creating and training a Logistic Regression model
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(max_iter=10000)
classifier.fit(X_train, y_train)

# Make predictions using test data
y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate performance metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

"""### DECISION TREE"""

# Creating and training a Decision Tree model
dt = DecisionTreeClassifier()

# Train the model
dt.fit(X_train, y_train)

# Make predictions using test data
y_pred_dt = dt.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_dt, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_dt, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))

"""### RANDOM FOREST"""

#Creating and training a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

#Make predictions using test data
rf_y_pred = rf.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, rf_y_pred))
print("Precision:", precision_score(y_test, rf_y_pred, average='macro'))
print("Recall:", recall_score(y_test, rf_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, rf_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, rf_y_pred))
print("Classification Report:")
print(classification_report(y_test, rf_y_pred))

"""### GRADIENT BOOSTING"""

# Creating and training a Gradient Boosting model
gb = GradientBoostingClassifier()

# Train the model
gb.fit(X_train, y_train)

#Make predictions using test data
y_pred_gb = gb.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print("Precision:", precision_score(y_test, y_pred_gb, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_gb, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_gb, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_gb))
print("Classification Report:")
print(classification_report(y_test, y_pred_gb))

"""### SUPPORT VECTOR MACHINE"""

# Creating and training a Support Vector Machine model
svm = SVC()

# Train the model
svm.fit(X_train, y_train)

#Make predictions using test data
y_pred_svm = svm.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Precision:", precision_score(y_test, y_pred_svm, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_svm, average='weighted'))
print("F1 score:", f1_score(y_test, y_pred_svm, average='weighted'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))

"""### KNN"""

#Creating and training a KNN model
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(X_train, y_train)

#Make predictions using test data
knn_y_pred = knn.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, knn_y_pred))
print("Precision:", precision_score(y_test, knn_y_pred, average='macro'))
print("Recall:", recall_score(y_test, knn_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, knn_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, knn_y_pred))
print("Classification Report:")
print(classification_report(y_test, knn_y_pred))

"""### NAIVE BAYES"""

# Creating and training a Naive Bayes model
nb = MultinomialNB()

# Train the model
nb.fit(X_train, y_train)


#Make predictions using test data
nb_y_pred = nb.predict(X_test)

# Calculate performance metrics

print("Accuracy:", accuracy_score(y_test, nb_y_pred))
print("Precision:", precision_score(y_test, nb_y_pred, average='macro'))
print("Recall:", recall_score(y_test, nb_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, nb_y_pred, average='macro'))

print("Confusion Matrix:")
print(confusion_matrix(y_test, nb_y_pred))
print("Classification Report:")
print(classification_report(y_test, nb_y_pred))