# -*- coding: utf-8 -*-
"""Berturk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KSFd5iK7VqkBBARYIPK72xC67UkNIx8a

# BerTurk

## Importing required libaries
"""

!pip install transformers

!pip install --upgrade transformers
!pip install transformers accelerate

!pip install datasets

!pip install pyarrow

import pandas as pd
import numpy as np
import torch
import tensorflow as tf
from nltk.corpus import stopwords
import torch.nn as nn
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from torch.optim import Adam
from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
import typing
from typing import Dict
import pyarrow as pa
from datasets import Dataset
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score

"""## Dataframe (clean_news.csv) processing-steps for training BerTURK Model"""

data_path = 'clean_news.csv'
df = pd.read_csv(data_path, error_bad_lines=False)

df.head()

df.shape

df1 = pd.DataFrame(df)
print(df1.count())

from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")
model = AutoModel.from_pretrained("dbmdz/bert-base-turkish-cased")

import re 
def process_data(row):
        # Clean the text
        text = row['Body']
        text = str(text)
        text = ' '.join(text.split())
        # Get tokens
        encodings = tokenizer(text, padding="max_length", truncation=True, max_length=128)
        # Convert string to integers
        label = 0
        if row['Label'] == 'gerçek':
            label += 1

        encodings['label'] = label
        encodings['text'] = text

        return encodings

print(process_data({
        'Body': 'this is a body text of news.',
        'Label': 'gerçek'
    }))

processed_data = []
for i in range(len(df[:4455])):
 processed_data.append(process_data(df.iloc[i]))

new_df = pd.DataFrame(processed_data)
  
train_df, valid_df = train_test_split(
        new_df,
        test_size=0.2,
        random_state=2022
    )

train_hg = Dataset(pa.Table.from_pandas(train_df))
valid_hg = Dataset(pa.Table.from_pandas(valid_df))

"""## Training Stage for BerTURK"""

def compute_metrics(p):
    print(type(p))
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
        'dbmdz/bert-base-turkish-cased',
        num_labels=2
    )

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="./result", evaluation_strategy="epoch", num_train_epochs= 5.0)

trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_hg,
        eval_dataset=valid_hg, 
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

trainer.train()

trainer.evaluate()